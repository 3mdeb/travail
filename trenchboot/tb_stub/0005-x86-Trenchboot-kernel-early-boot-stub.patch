From 286451433c4d988e2d1d52795e7f5bf96cbea1f3 Mon Sep 17 00:00:00 2001
From: Ross Philipson <ross.philipson@oracle.com>
Date: Mon, 10 Dec 2018 15:04:48 -0500
Subject: [PATCH 5/8] x86: Trenchboot kernel early boot stub

Signed-off-by: Ross Philipson <ross.philipson@oracle.com>
---
 arch/x86/boot/compressed/Makefile  |   2 +
 arch/x86/boot/compressed/head_64.S |  16 ++++
 arch/x86/boot/compressed/tb_stub.S | 179 +++++++++++++++++++++++++++++++++++++
 3 files changed, 197 insertions(+)
 create mode 100644 arch/x86/boot/compressed/tb_stub.S

diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 466f66c8a7f8..f28c4e7ec585 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -90,6 +90,8 @@ vmlinux-objs-$(CONFIG_EFI_STUB) += $(obj)/eboot.o $(obj)/efi_stub_$(BITS).o \
 	$(objtree)/drivers/firmware/efi/libstub/lib.a
 vmlinux-objs-$(CONFIG_EFI_MIXED) += $(obj)/efi_thunk_$(BITS).o
 
+vmlinux-objs-$(CONFIG_TRENCHBOOT_STUB) += $(obj)/tb_stub.o
+
 # The compressed kernel is built with -fPIC/-fPIE so that a boot loader
 # can place it anywhere in memory and it will still run. However, since
 # it is executed as-is without any ELF relocation processing performed
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index 64037895b085..687ad217bcc3 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -248,6 +248,22 @@ ENTRY(efi32_stub_entry)
 ENDPROC(efi32_stub_entry)
 #endif
 
+#ifdef CONFIG_TRENCHBOOT_STUB
+ENTRY(tb_stub_entry)
+	/*
+	 * On entry, %ebx has the entry abs offset to tb_stub_entry. To
+	 * find the beginning of where we are loaded, sub off from the
+	 * beginning.
+	 */
+	movl	%ebx, %ebp
+	subl	$(tb_stub_entry - startup_32), %ebp
+
+	/* More room to work in tb_stub in the text section */
+	jmp	tb_stub
+
+ENDPROC(tb_stub_entry)
+#endif
+
 	.code64
 	.org 0x200
 ENTRY(startup_64)
diff --git a/arch/x86/boot/compressed/tb_stub.S b/arch/x86/boot/compressed/tb_stub.S
new file mode 100644
index 000000000000..fcb9bb6cbdc3
--- /dev/null
+++ b/arch/x86/boot/compressed/tb_stub.S
@@ -0,0 +1,179 @@
+/*
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+ */
+	.code32
+	.text
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/msr.h>
+#include <asm/processor-flags.h>
+#include <asm/asm-offsets.h>
+#include <asm/bootparam.h>
+#include <asm/trenchboot.h>
+
+	/* The MLE Header per the TXT Specification, section 4.1 */
+	.global	tb_mle_header
+tb_mle_header:
+	.long	0x9082ac5a /* UUID0 */
+	.long	0x74a7476f /* UUID1 */
+	.long	0xa2555c0f /* UUID2 */
+	.long	0x42b651cb /* UUID3 */
+	.long	0x00000034 /* MLE header size */
+	.long	0x00020002 /* MLE version 2.2 */
+	.long	0x00000000 /* Linear entry point of MLE */
+	.long	0x00000000 /* First valid page of MLE */
+	.long	0x00000000 /* Offset within binary of first byte of MLE */
+	.long	0x00000000 /* Offset within binary of last byte + 1 of MLE */
+	.long	0x00000223 /* Bit vector of MLE-supported capabilities */
+	.long	0x00000000 /* Starting linear address of command line */
+	.long	0x00000000 /* Ending linear address of command line */
+
+	.code32
+ENTRY(tb_stub)
+	cli
+	cld
+
+	/* On entry %ebp has the base address from head_64.S, load the stack */
+	leal	tb_stack_end(%ebp), %esp
+
+	/* TB flag indicates TB entry mode */
+	andl	$(TB_FLAG_ACTIVE), tb_flags(%ebp)
+
+	/* On entry, only %cs is known good */
+	addl	%ebp, (tb_gdt + 2)(%ebp)
+	lgdt	tb_gdt(%ebp)
+
+	movl	$(__TB32_DS), %eax
+	movw	%ax, %ds
+	movw	%ax, %es
+	movw	%ax, %fs
+	movw	%ax, %gs
+	movw	%ax, %ss
+
+	leal	.Ltb_cs(%ebp), %eax
+	pushl	$(__TB32_CS)
+	pushl	%eax
+	lret
+
+.Ltb_cs:
+	addl	$8, %esp
+
+	/* Before going any further, make sure this is the BSP */
+	movl	$(MSR_IA32_APICBASE), %ecx
+	rdmsr
+	testl	$(MSR_IA32_APICBASE_BSP), %eax
+	jnz	.Lbsp_ok
+	ud2
+
+.Lbsp_ok:
+	/* Assume CPU is AMD to start */
+	andl	$(CPU_AMD), %edi
+
+	/* Now see if it is Intel */
+	movl	$0x0, %eax
+	cpuid
+	cmpl	$0x756e6547, %ebx # GenuineIntel?
+	jnz	.Lcpu_check_done
+	cmpl	$0x49656e69, %edx
+	jnz	.Lcpu_check_done
+	cmpl	$0x6c65746e, %ecx
+	jnz	.Lcpu_check_done
+	andl	$(CPU_INTEL), %edi
+
+.Lcpu_check_done:
+	/* Now that we know what CPU it is, do vendor specific operations */
+	testl	$(CPU_AMD), %edi
+	jz	.Ldo_amd
+
+	/* Enable SMI with GETSET[SMCTRL] */
+	xorl	%ebx, %ebx
+	movl	$(X86_GETSEC_SMCTRL), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+	/* An IRET-to-self can be used to unmask NMIs which SENTER masked */
+	leal	.Lnmi_enabled(%ebp), %eax
+	pushfl
+	pushl	$(__TB32_CS)
+	pushl	%eax
+	iret
+
+.Lnmi_enabled:
+	addl	$12, %esp
+
+	/* On Intel, the zero page address is passed in the TXT heap */
+	movl	$(TXT_PUB_CONFIG_REGS_BASE), %eax /* TXT MMIO pub regs */
+	movl	TXTCR_HEAP_BASE(%eax), %ecx /* read size of BIOS data */
+	addl	%ecx, %eax /* skip over BIOS data to OS-MLE data */
+	movl	(%eax), %esi /* read zero page addr into %esi */
+
+	/* Clear the TXT error registers for a clean start of day */
+	movl	$(TXT_PRIV_CONFIG_REGS_BASE), %eax /* TXT MMIO priv regs */
+	movl	$0x0, TXTCR_ERRORCODE(%eax)
+	movl	$0xffffffff, TXTCR_ESTS(%eax)
+
+	andl	$(TB_FLAG_ARCH_TXT), tb_flags(%ebp)
+
+	jmp	.Lcpu_setup_done
+
+.Ldo_amd:
+	/*
+	 * Disable maskable interrups in EFLAGS then enable global interrupts
+	 * including SMI and NMI (GIF).
+	 */
+	cli
+	stgi
+
+	/* On AMD %esi is set up by the Landing Zone, just go on */
+	andl	$(TB_FLAG_ARCH_SKINIT), tb_flags(%ebp)
+
+.Lcpu_setup_done:
+	/*
+	 * Don't enable MCE at this point. The kernel will enable
+	 * it on the BSP later when it is ready.
+	 */
+
+	/* Store flags in boot params for later use */
+	movl	%eax, (TRENCHBOOT_INFO_OFFSET + TB_FLAGS_OFFSET)(%esi)
+
+	/* Don't want the kernel keeping any segment from the TB stub */
+	andb	$~(KEEP_SEGMENTS), BP_loadflags(%esi)
+
+	/* Done, jump to normal 32b pm entry */
+	jmp	startup_32
+ENDPROC(tb_stub)
+
+	.data
+	.balign 16
+tb_gdt:
+	.word	tb_gdt_end - tb_gdt - 1
+	.long	tb_gdt
+	.word	0
+	.quad	0x00cf9a000000ffff	/* __TB32_CS */
+	.quad	0x00cf92000000ffff	/* __TB32_DS */
+tb_gdt_end:
+
+	/*
+	 * Temporary storage for flags until they can be loaded into
+	 * boot params making them available to the real kernel.
+	 */
+	.balign 4
+tb_flags:
+	.long	0
+
+	/* Small stack to work with */
+	.balign 4
+tb_stack:
+	.fill 32, 1, 0
+tb_stack_end:
-- 
2.13.6

