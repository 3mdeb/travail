From 8d4b06216cddb06f8061058653fe6c6f67a4524f Mon Sep 17 00:00:00 2001
From: Ross Philipson <ross.philipson@oracle.com>
Date: Fri, 26 Oct 2018 15:53:24 -0400
Subject: [PATCH 5/9] x86: Trenchboot kernel early boot stub

Signed-off-by: Ross Philipson <ross.philipson@oracle.com>
---
 arch/x86/boot/compressed/Makefile  |   2 +-
 arch/x86/boot/compressed/head_64.S |  46 ++++
 arch/x86/boot/compressed/sl_main.c | 202 +++++++++++++++
 arch/x86/boot/compressed/sl_stub.S | 488 +++++++++++++++++++++++++++++++++++++
 4 files changed, 737 insertions(+), 1 deletion(-)
 create mode 100644 arch/x86/boot/compressed/sl_main.c
 create mode 100644 arch/x86/boot/compressed/sl_stub.S

diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 1a089f7f7f79..50f1ff478db0 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -91,7 +91,7 @@ vmlinux-objs-$(CONFIG_EFI_STUB) += $(obj)/eboot.o $(obj)/efi_stub_$(BITS).o \
 vmlinux-objs-$(CONFIG_EFI_MIXED) += $(obj)/efi_thunk_$(BITS).o
 
 vmlinux-objs-$(CONFIG_SECURE_LAUNCH_STUB) += $(obj)/early_sha1.o \
-	$(obj)/early_tpm.o
+	$(obj)/early_tpm.o $(obj)/sl_main.o $(obj)/sl_stub.o
 
 # The compressed kernel is built with -fPIC/-fPIE so that a boot loader
 # can place it anywhere in memory and it will still run. However, since
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index 64037895b085..4a0c76ecce1a 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -248,6 +248,22 @@ ENTRY(efi32_stub_entry)
 ENDPROC(efi32_stub_entry)
 #endif
 
+#ifdef CONFIG_SECURE_LAUNCH_STUB
+ENTRY(sl_stub_entry)
+	/*
+	 * On entry, %ebx has the entry abs offset to sl_stub_entry. To
+	 * find the beginning of where we are loaded, sub off from the
+	 * beginning.
+	 */
+	movl	%ebx, %ebp
+	subl	$(sl_stub_entry - startup_32), %ebp
+
+	/* More room to work in sl_stub in the text section */
+	jmp	sl_stub
+
+ENDPROC(sl_stub_entry)
+#endif
+
 	.code64
 	.org 0x200
 ENTRY(startup_64)
@@ -520,6 +536,36 @@ relocated:
 	shrq	$3, %rcx
 	rep	stosq
 
+#ifdef CONFIG_SECURE_LAUNCH_STUB
+	/* Disable all caching for MMIO operations */
+	pushq	%rax
+	movq	%cr0, %rax
+	pushq	%rax
+	orq	$(X86_CR0_CD), %rax
+	andq	$(~X86_CR0_NW), %rax
+	movq	%rax, %cr0
+	wbinvd
+
+	/*
+	 * Have to do the final early sl stub work in 64b area.
+	 *
+	 * *********** NOTE ***********
+	 *
+	 * Several boot params get used before we get a chance to measure
+	 * them in this call. This is a known issue and we currently don't
+	 * have a solution. One solution might be to set them in the really
+	 * early sl stub asm code but that might not work well.
+	 */
+	pushq	%rsi
+	movq	%rsi, %rdi
+	callq	sl_main
+	popq	%rsi
+
+	popq	%rax
+	movq	%rax, %cr0
+	popq	%rax
+#endif
+
 /*
  * Do the extraction, and jump to the new kernel..
  */
diff --git a/arch/x86/boot/compressed/sl_main.c b/arch/x86/boot/compressed/sl_main.c
new file mode 100644
index 000000000000..66c0e90beb74
--- /dev/null
+++ b/arch/x86/boot/compressed/sl_main.c
@@ -0,0 +1,202 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.
+ */
+
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/boot.h>
+#include <asm/msr.h>
+#include <asm/mtrr.h>
+#include <asm/processor-flags.h>
+#include <asm/asm-offsets.h>
+#include <asm/sha1.h>
+#include <asm/tpm.h>
+#include <asm/bootparam.h>
+#include <asm/slaunch.h>
+
+extern u32 sl_cpu_type;
+
+static u64 sl_txt_read(u32 reg)
+{
+	void *addr = (void*)(u64)(TXT_PRIV_CONFIG_REGS_BASE + reg);
+	u64 val;
+
+	barrier();
+	val = (*(volatile u64*)(addr));
+	rmb();
+
+	return val;
+}
+
+static void sl_txt_write(u32 reg, u64 val)
+{
+	void *addr = (void*)(u64)(TXT_PRIV_CONFIG_REGS_BASE + reg);
+
+	barrier();
+	(*(volatile u64 *)(addr)) = val;
+	wmb();
+	barrier();
+}
+
+static void sl_txt_reset(u64 error)
+{
+	sl_txt_write(TXTCR_ERRORCODE, error);
+	(void)sl_txt_read(TXTCR_E2STS);
+	sl_txt_write(TXTCR_CMD_UNLOCK_MEM_CONFIG, 1);
+	(void)sl_txt_read(TXTCR_E2STS);
+	sl_txt_write(TXTCR_CMD_RESET, 1);
+	for ( ; ; )
+		__asm__ __volatile__ ("pause");
+}
+
+static void sl_skinit_reset(void)
+{
+	/* TODO not sure what else to do here. Is there an error reg */
+	__asm__ __volatile__ ("ud2");
+}
+
+static u64 sl_rdmsr(u32 reg)
+{
+	u32 lo, hi;
+	u64 val;
+
+	__asm__ __volatile__ ("rdmsr"  : "=a" (lo), "=d" (hi) : "c" (reg));
+	val = hi;
+	return ((val << 32) & 0xffffffff00000000) | lo;
+}
+
+static void sl_txt_validate_msrs(struct txt_os_mle_data *os_mle_data)
+{
+#define CAPS_VARIABLE_MTRR_COUNT_MASK   0xff
+	u64 mtrr_caps, mtrr_def_type, mtrr_var, misc_en_msr;
+	u32 vcnt, i;
+	struct txt_mtrr_state *saved_bsp_mtrrs =
+		&(os_mle_data->saved_bsp_mtrrs);
+
+	mtrr_caps = sl_rdmsr(MSR_MTRRcap);
+	vcnt = (u32)(mtrr_caps & CAPS_VARIABLE_MTRR_COUNT_MASK);
+
+	if (saved_bsp_mtrrs->mtrr_vcnt > vcnt)
+		sl_txt_reset(TXT_SLERROR_MTRR_INV_VCNT);
+	if (saved_bsp_mtrrs->mtrr_vcnt > TXT_MAX_VARIABLE_MTRRS)
+		sl_txt_reset(TXT_SLERROR_MTRR_INV_VCNT);
+
+	mtrr_def_type = sl_rdmsr(MSR_MTRRdefType);
+	if (saved_bsp_mtrrs->default_type_reg != mtrr_def_type)
+		sl_txt_reset(TXT_SLERROR_MTRR_INV_DEF_TYPE);
+
+	for (i = 0; i < saved_bsp_mtrrs->mtrr_vcnt; i++) {
+		mtrr_var = sl_rdmsr(MTRRphysBase_MSR(i));
+		if (saved_bsp_mtrrs->mtrr_pair[i].mtrr_physbase != mtrr_var)
+			sl_txt_reset(TXT_SLERROR_MTRR_INV_BASE);
+		mtrr_var = sl_rdmsr(MTRRphysMask_MSR(i));
+		if (saved_bsp_mtrrs->mtrr_pair[i].mtrr_physmask != mtrr_var)
+			sl_txt_reset(TXT_SLERROR_MTRR_INV_MASK);
+	}
+
+	misc_en_msr = sl_rdmsr(MSR_IA32_MISC_ENABLE);
+	if (os_mle_data->saved_misc_enable_msr != misc_en_msr)
+		sl_txt_reset(TXT_SLERROR_MSR_INV_MISC_EN);
+}
+
+void sl_main(u8 *bootparams)
+{
+	struct sha1_state sctx = {0};
+	u8 sha1_hash[SHA1_DIGEST_SIZE];
+	u32 cmdline_len;
+	u64 cmdline_addr;
+	struct tpm *tpm;
+	int ret;
+
+	memset(sha1_hash, 0, SHA1_DIGEST_SIZE);
+
+	/*
+	 * If enable_tpm fails there is no point going on. The entire secure
+	 * environment depends on this and the other TPM operations succeeding.
+	 */
+	tpm = enable_tpm();
+	if (!tpm) {
+		if (sl_cpu_type == SL_CPU_INTEL)
+			sl_txt_reset(TXT_SLERROR_TPM_INIT);
+		else
+			sl_skinit_reset();
+	}
+
+	if (tpm_request_locality(tpm, 2) == TPM_NO_LOCALITY) {
+		if (sl_cpu_type == SL_CPU_INTEL)
+			sl_txt_reset(TXT_SLERROR_TPM_GET_LOC);
+		else
+			sl_skinit_reset();
+	}
+
+	/* Hash the zero page/boot params */
+	early_sha1_init(&sctx);
+	early_sha1_update(&sctx, bootparams, PAGE_SIZE);
+	early_sha1_finalize(&sctx);
+	early_sha1_finish(&sctx, &sha1_hash[0]);
+	ret = tpm_extend_pcr(tpm, 18, TPM_HASH_ALG_SHA1, &sha1_hash[0]);
+	if (ret) {
+		if (sl_cpu_type == SL_CPU_INTEL)
+			sl_txt_reset(TXT_SLERROR_TPM_EXTEND);
+		else
+			sl_skinit_reset();
+	}
+
+	memset(sha1_hash, 0, SHA1_DIGEST_SIZE);
+	cmdline_len = *((u32*)(bootparams + BP_CMDLINE_SIZE));
+	cmdline_addr = *((u32*)(bootparams + BP_CMD_LINE_PTR));
+
+	/* Hash the command line */
+	early_sha1_init(&sctx);
+	early_sha1_update(&sctx, (u8*)cmdline_addr, cmdline_len);
+	early_sha1_finalize(&sctx);
+	early_sha1_finish(&sctx, &sha1_hash[0]);
+	ret = tpm_extend_pcr(tpm, 18, TPM_HASH_ALG_SHA1, &sha1_hash[0]);
+	if (ret) {
+		if (sl_cpu_type == SL_CPU_INTEL)
+			sl_txt_reset(TXT_SLERROR_TPM_EXTEND);
+		else
+			sl_skinit_reset();
+	}
+
+	tpm_relinquish_locality(tpm);
+	free_tpm(tpm);
+
+	if (sl_cpu_type == SL_CPU_INTEL) {
+		struct txt_os_mle_data *os_mle_data;
+		u64 *txt_heap;
+		u64 bios_data_size;
+		u32 os_mle_len;
+
+		/*
+		 * Some extra work to do on Intel, have to measure the OS-MLE
+		 * heap area.
+		 */
+		txt_heap = (void*)sl_txt_read(TXTCR_HEAP_BASE);
+		bios_data_size = *txt_heap;
+		os_mle_data = (struct txt_os_mle_data*)
+				((u8*)txt_heap + bios_data_size + sizeof(u64));
+
+		/* Measure OS-MLE data up to the TPM log into 18 */
+		os_mle_len = offsetof(struct txt_os_mle_data, event_log_buffer);
+		early_sha1_init(&sctx);
+		early_sha1_update(&sctx, (u8*)os_mle_data, os_mle_len);
+		early_sha1_finalize(&sctx);
+		early_sha1_finish(&sctx, &sha1_hash[0]);
+		ret = tpm_extend_pcr(tpm, 18, TPM_HASH_ALG_SHA1, &sha1_hash[0]);
+		if (ret) {
+			if (sl_cpu_type == SL_CPU_INTEL)
+				sl_txt_reset(TXT_SLERROR_TPM_EXTEND);
+			else
+				sl_skinit_reset();
+		}
+
+		/*
+		 * Now that the OS-MLE data is measured, ensure the MTRR and
+		 * misc enable MSRs are what we expect.
+		 */
+		sl_txt_validate_msrs(os_mle_data);
+	}
+}
diff --git a/arch/x86/boot/compressed/sl_stub.S b/arch/x86/boot/compressed/sl_stub.S
new file mode 100644
index 000000000000..00463adc5909
--- /dev/null
+++ b/arch/x86/boot/compressed/sl_stub.S
@@ -0,0 +1,488 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+/*
+ * Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.
+ *
+ * Author(s):
+ *     Ross Philipson <ross.philipson@oracle.com>
+ */
+	.code32
+	.text
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/msr.h>
+#include <asm/processor-flags.h>
+#include <asm/asm-offsets.h>
+#include <asm/bootparam.h>
+#include <asm/irq_vectors.h>
+#include <asm/slaunch.h>
+
+/* Can't include apiddef.h in asm */
+#define	APIC_EOI	0xB0
+#define	APIC_EOI_ACK	0x0
+
+	/* The MLE Header per the TXT Specification, section 4.1 */
+	.global	sl_mle_header
+sl_mle_header:
+	.long	0x9082ac5a    /* UUID0 */
+	.long	0x74a7476f    /* UUID1 */
+	.long	0xa2555c0f    /* UUID2 */
+	.long	0x42b651cb    /* UUID3 */
+	.long	0x00000034    /* MLE header size */
+	.long	0x00020002    /* MLE version 2.2 */
+	.long	sl_stub_entry /* Linear entry point of MLE (virt. address) */
+	.long	0x00000000    /* First valid page of MLE */
+	.long	0x00000000    /* Offset within binary of first byte of MLE */
+	.long	0x00000000    /* Offset within binary of last byte + 1 of MLE */
+	.long	0x00000223    /* Bit vector of MLE-supported capabilities */
+	.long	0x00000000    /* Starting linear address of command line */
+	.long	0x00000000    /* Ending linear address of command line */
+
+	.code32
+ENTRY(sl_stub)
+	/*
+	 * On entry, %ebp has the base address from head_64.S
+	 * and only %cs is known good
+	 */
+	cli
+	cld
+
+	/*
+	 * Take the first stack for the BSP. The AP stacks are only used
+	 * on Intel.
+	 */
+	leal	sl_stacks_end(%ebp), %esp
+
+	addl	%ebp, (sl_gdt + 2)(%ebp)
+	lgdt	sl_gdt(%ebp)
+
+	movl	$(__SL32_DS), %eax
+	movw	%ax, %ds
+	movw	%ax, %es
+	movw	%ax, %fs
+	movw	%ax, %gs
+	movw	%ax, %ss
+
+	leal	.Lsl_cs(%ebp), %eax
+	pushl	$(__SL32_CS)
+	pushl	%eax
+	lret
+
+.Lsl_cs:
+	addl	$8, %esp
+
+	/* Before going any further, make sure this is the BSP */
+	movl	$(MSR_IA32_APICBASE), %ecx
+	rdmsr
+	testl	$(MSR_IA32_APICBASE_BSP), %eax
+	jnz	.Lbsp_ok
+	ud2
+
+.Lbsp_ok:
+	/* Assume CPU is AMD to start */
+	movl	$(SL_CPU_AMD), %edi
+
+	/* Now see if it is Intel */
+	movl	$0x0, %eax
+	cpuid
+	cmpl	$0x756e6547, %ebx # GenuineIntel?
+	jnz	.Lcpu_check_done
+	cmpl	$0x49656e69, %edx
+	jnz	.Lcpu_check_done
+	cmpl	$0x6c65746e, %ecx
+	jnz	.Lcpu_check_done
+	movl	$(SL_CPU_INTEL), %edi
+
+.Lcpu_check_done:
+	/* Now that we know what CPU it is, do vendor specific operations */
+	cmpl	$(SL_CPU_AMD), %edi
+	jz	.Ldo_amd
+
+	/* Know it is Intel */
+	movl	$(SL_CPU_INTEL), sl_cpu_type(%ebp)
+
+	/* Increment CPU count for BSP */
+	incl	sl_txt_cpu_count(%ebp)
+
+	/* Enable SMI with GETSET[SMCTRL] */
+	xorl	%ebx, %ebx
+	movl	$(TXT_X86_GETSEC_SMCTRL), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+	/* An IRET-to-self can be used to unmask NMIs which SENTER masked */
+	leal	.Lnmi_enabled(%ebp), %eax
+	pushfl
+	pushl	$(__SL32_CS)
+	pushl	%eax
+	iret
+
+.Lnmi_enabled:
+	addl	$12, %esp
+
+	/* Clear the TXT error registers for a clean start of day */
+	movl	$0, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_ERRORCODE)
+	movl	$0xffffffff, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_ESTS)
+
+	/* On Intel, the zero page address is passed in the TXT heap */
+	/* Read physical base of heap into EAX */
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_HEAP_BASE), %eax
+	/* Read the size of the BIOS data into ECX (in first 8 bytes */
+	movl	(%eax), %ecx
+	/* Skip over BIOS data and size of OS to MLE */
+	addl	%ecx, %eax
+	addl	$8, %eax
+	/* First 4 bytes of OS to MLE are the version */
+	/* Second 4 bytes of OS to MLE are the zero page */
+	movl	TXT_OS_MLE_ZERO_PAGE_ADDR(%eax), %esi
+
+	/* Save ebp so the APs can find their way home */
+	movl	%ebp, TXT_OS_MLE_AP_WAKE_EBP(%eax)
+
+	/* Note only %esi and %ebp MUST be preserved across calls */
+	movl	%eax, %edi
+	call	sl_txt_load_regs
+
+	/* Wake up all APs and wait for them to halt */
+	call	sl_txt_wake_aps
+
+	jmp	.Lcpu_setup_done
+
+.Ldo_amd:
+	/* Know it is AMD */
+	movl	$(SL_CPU_AMD), sl_cpu_type(%ebp)
+
+	/*
+	 * Disable maskable interrups in EFLAGS then enable global interrupts
+	 * including SMI and NMI (GIF).
+	 */
+	cli
+	stgi
+
+	/* On AMD %esi is set up by the Landing Zone, just go on */
+
+.Lcpu_setup_done:
+	/*
+	 * Don't enable MCE at this point. The kernel will enable
+	 * it on the BSP later when it is ready.
+	 */
+
+	/* Keep SL segments for the early portion of the kernel boot */
+	orb	$(KEEP_SEGMENTS), BP_loadflags(%esi)
+
+	/* Done, jump to normal 32b pm entry */
+	jmp	startup_32
+ENDPROC(sl_stub)
+
+ENTRY(sl_txt_ap_entry)
+	cli
+	cld
+
+	/*
+	 * TXT has kindly loaded sl_gdt into GDTR and setup the segment
+	 * register. First order of business is to find where we are and
+	 * save it in ebp.
+	 */
+
+	/* Read physical base of heap into EAX */
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_HEAP_BASE), %eax
+	/* Read the size of the BIOS data into ECX (in first 8 bytes */
+	movl	(%eax), %ecx
+	/* Skip over BIOS data and size of OS to MLE */
+	addl	%ecx, %eax
+	addl	$8, %eax
+
+	/* Saved ebp from the BSP and stash OS-MLE pointer */
+	movl	TXT_OS_MLE_AP_WAKE_EBP(%eax), %ebp
+	movl	%eax, %edi
+
+	/* Setup our IDT */
+	leal	sl_txt_int_reset(%ebp), %eax
+	leal	sl_txt_int_ipi_wake(%ebp), %ebx
+	leal	sl_idt_table(%ebp), %ecx
+	xorl	%edx, %edx
+
+
+1:
+	cmpw	$(NR_VECTORS), %dx
+	jnz	1f
+
+	cmpw	$(TXT_AP_WAKE_IPI_VEC), %dx
+	jz	2f
+	/* Load default reset vector */
+	movl	%eax, %esi
+	movw	%si, (%ecx)
+	shrl	$16, %esi
+	movw	%si, 6(%ecx)
+	jmp	3f
+
+2:
+	/* Load wake IPI vector */
+	movl	%ebx, %esi
+	movw	%si, (%ecx)
+	shrl	$16, %esi
+	movw	%si, 6(%ecx)
+
+3:
+	cmpw	$(MCE_VECTOR), %dx
+	jnz	4f
+
+	/* Fixup MCE vectore to be a trap */
+	movw	$0x8f00, 4(%ecx) /* Present, DPL=0, 32b Vector, Trap */
+
+4:
+	incw	%dx
+	addl	$8, %ecx
+	jmp	1b
+
+1:
+	addl	%ebp, (sl_idt_desc + 2)(%ebp)
+	lidt	sl_idt_desc(%ebp)
+
+	/* Lock and get our stack index */
+	movl	$1, %ecx
+.Lspin:
+	xorl	%eax, %eax
+	lock cmpxchgl	%ecx, sl_txt_spin_lock(%ebp)
+	jnz	.Lspin
+
+	leal	sl_txt_stack_index(%ebp), %ebx
+	movl	(%ebx), %eax
+	incl	%eax
+	movl	%eax, (%ebx)
+
+	/* Unlock */
+	movl	$0, sl_txt_spin_lock(%ebp)
+
+	/* Load our AP stack */
+	movl	$(TXT_BOOT_STACK_SIZE), %edx
+	mull	%edx
+	leal	sl_stacks_end(%ebp), %edx
+	subl	%eax, %edx
+	movl	%edx, %esp
+
+	/* Enable SMI with GETSET[SMCTRL] */
+	xorl	%ebx, %ebx
+	movl	$(TXT_X86_GETSEC_SMCTRL), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+	/* An IRET-to-self can be used to unmask NMIs which SENTER masked */
+	leal	.Lnmi_enabled_ap(%ebp), %eax
+	pushfl
+	pushl	$(__SL32_CS)
+	pushl	%eax
+	iret
+
+.Lnmi_enabled_ap:
+	addl	$12, %esp
+
+	/* Fixup MTRRs and misc enable MSR on APs too */
+	call	sl_txt_load_regs
+
+	/* Read MSR to determine if it will be MWAIT or HLT */
+	movl	$(MSR_IA32_MISC_ENABLE), %ecx
+	rdmsr
+
+	/* Basically done, increment the CPU count and wait for IPI */
+	xorl	%ebx, %ebx
+	sti
+	lock incl	sl_txt_cpu_count(%ebp)
+
+1:
+	cmpl	$0, %ebx
+	jnz	2f
+	pause
+	jmp	1b
+2:
+ENDPROC(sl_txt_ap_entry)
+
+ENTRY(sl_txt_load_regs)
+	/*
+	 * On Intel, the original variable MTRRs and Misc Enable MSR are
+	 * restored on the BSP at early boot. Each AP will also restore
+	 * its MTRRs and Misc Enable MSR.
+	 */
+	pushl	%edi
+	addl	$(TXT_OS_MLE_MTRR_STATE), %edi
+	movl	(%edi), %ebx
+	pushl	%ebx /* default_type_reg lo */
+	addl	$4, %edi
+	movl	(%edi), %ebx
+	pushl	%ebx /* default_type_reg hi */
+	addl	$4, %edi
+	movl	(%edi), %ebx /* mtrr_vcnt lo, don't care about hi part */
+	addl	$8, %edi /* now at MTRR pair array */
+	/* Write the variable MTRRs */
+	movl	$(MTRRphysBase0), %ecx
+1:
+	cmpl	$0, %ebx
+	jz	2f
+
+	movl	(%edi), %eax /* MTRRphysBaseX lo */
+	addl	$4, %edi
+	movl	(%edi), %edx /* MTRRphysBaseX hi */
+	wrmsr
+	addl	$4, %edi
+	incl	%ecx
+	movl	(%edi), %eax /* MTRRphysMaskX lo */
+	addl	$4, %edi
+	movl	(%edi), %edx /* MTRRphysMaskX hi */
+	wrmsr
+	addl	$4, %edi
+	incl	%ecx
+
+	decl	%ebx
+	jmp	1b
+2:
+	/* Write the default MTRR register */
+	popl	%edx
+	popl	%eax
+	movl	$(MSR_MTRRdefType), %ecx
+	wrmsr
+
+	/* Return to beginning and write the misc enable msr */
+	popl	%edi
+	addl	$(TXT_OS_MLE_MISC_EN_MSR), %edi
+	movl	(%edi), %eax /* saved_misc_enable_msr lo */
+	addl	$4, %edi
+	movl	(%edi), %edx /* saved_misc_enable_msr hi */
+	movl	$(MSR_IA32_MISC_ENABLE), %ecx
+	wrmsr
+
+	ret
+ENDPROC(sl_txt_load_regs)
+
+ENTRY(sl_txt_wake_aps)
+	/* First setup the MLE join structure and load it into TXT reg */
+	leal	sl_gdt(%ebp), %eax
+	leal	sl_txt_ap_entry(%ebp), %ecx
+	leal	sl_txt_mle_join(%ebp), %edx
+	movl	%eax, 4(%edx)
+	movl	%ecx, 12(%edx)
+	movl	%edx, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_MLE_JOIN)
+
+	/* Another TXT heap walk to find various values needed to wake APs */
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_HEAP_BASE), %eax
+	/* At BIOS data size, find the number of logical processors */
+	movl	(TXT_BIOS_NUM_LOG_PROCS + 8)(%eax), %edx
+	/* Skip over BIOS data */
+	movl	(%eax), %ecx
+	addl	%ecx, %eax
+	/* Skip over OS to MLE */
+	movl	(%eax), %ecx
+	addl	%ecx, %eax
+	/* At OS-SNIT size, get capabilities to know how to wake up the APs */
+	movl	(TXT_OS_SINIT_CAPABILITIES + 8)(%eax), %ebx
+	/* Skip over OS to SNIT */
+	movl	(%eax), %ecx
+	addl	%ecx, %eax
+	/* At SNIT-MLE size, get the AP wake MONITOR address */
+	movl	(TXT_SINIT_MLE_RLP_WAKEUP_ADDR + 8)(%eax), %edi
+
+	/* Determine how to wake up the APs */
+	testl	$(1 << TXT_SINIT_CAPS_WAKE_MONITOR), %ebx
+	jz	.Lwake_getsec
+
+	/* Wake using MWAIT MONITOR */
+	movl	$1, (%edi)
+	jmp	.Laps_awake
+
+.Lwake_getsec:
+	/* Wake using GETSEC(WAKEUP) */
+	xorl	%ebx, %ebx
+	movl	$(TXT_X86_GETSEC_WAKEUP), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+.Laps_awake:
+	/* Wait for all of them to halt */
+1:
+	cmpl	sl_txt_cpu_count(%ebp), %edx
+	jz	2f
+	pause
+	jmp	1b
+
+2:
+	ret
+ENDPROC(sl_txt_wake_aps)
+
+ENTRY(sl_txt_int_ipi_wake)
+	movl	$1, %ebx
+
+	movl	$(MSR_IA32_APICBASE), %ecx
+	rdmsr
+	andl	$(MSR_IA32_APICBASE_BASE), %eax
+	addl	$(APIC_EOI), %eax
+	movl	$(APIC_EOI_ACK), (%eax)
+
+	iret
+ENDPROC(sl_txt_int_ipi_wake)
+
+ENTRY(sl_txt_int_reset)
+	movl	$(TXT_SLERROR_INV_AP_INTERRUPT), (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_ERRORCODE)
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_E2STS), %eax
+	movl	$1, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_CMD_UNLOCK_MEM_CONFIG)
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_E2STS), %eax
+	movl	$1, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_CMD_RESET)
+1:
+	pause
+	jmp 	1b
+ENDPROC(sl_txt_int_reset)
+
+	.data
+	.balign 4096
+sl_gdt:
+	.word	sl_gdt_end - sl_gdt - 1
+	.long	sl_gdt
+	.word	0x0000
+	.quad	0x00cf9a000000ffff	/* __SL32_CS */
+	.quad	0x00cf92000000ffff	/* __SL32_DS */
+	.quad	0x008f9a000000ffff	/* __SL16_CS */
+	.quad	0x008f92000000ffff	/* __SL16_DS */
+sl_gdt_end:
+
+	.balign 16
+sl_idt_desc:
+	.word	sl_idt_table_end - sl_idt_table - 1 /* Limit */
+	.long	sl_idt_table			    /* Base */
+sl_idt_desc_end:
+
+	.balign 16
+sl_idt_table:
+	.rept	NR_VECTORS
+	.word	0x0000		/* Offset 15 to 0 */
+	.word	__SL32_CS	/* Segment selector */
+	.word	0x8e00		/* Present, DPL=0, 32b Vector, Interrupt */
+	.word	0x0000		/* Offset 31 to 16 */
+	.endr
+sl_idt_table_end:
+
+	.balign	16
+sl_rm_idt_desc:
+	.word	(0x400 - 1)	/* 256 4b entries */
+	.long	0
+sl_rm_idt_desc_end:
+
+	.balign 16
+sl_txt_mle_join:
+	.long	sl_gdt_end - sl_gdt - 1	/* GDT limit */
+	.long	0x00000000		/* GDT base */
+	.long	__SL32_CS	/* Seg Sel - CS (DS, ES, SS = seg_sel+8) */
+	.long	0x00000000	/* Entry point physical address */
+
+	.global	sl_cpu_type
+sl_cpu_type:
+	.long	0x00000000
+
+sl_txt_spin_lock:
+	.long	0x00000000
+
+sl_txt_stack_index:
+	.long	0x00000000
+
+sl_txt_cpu_count:
+	.long	0x00000000
+
+	/* Small stacks for BSP and APs to work with */
+	.balign 4
+sl_stacks:
+	.fill (TXT_MAX_CPUS*TXT_BOOT_STACK_SIZE), 1, 0
+sl_stacks_end:
-- 
2.13.6

