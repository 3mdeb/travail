From 4861a916402db07f48a34162fb03509ca062bcce Mon Sep 17 00:00:00 2001
From: Ross Philipson <ross.philipson@oracle.com>
Date: Fri, 21 Sep 2018 10:24:56 -0400
Subject: [PATCH 2/5] x86: Trenchboot kernel early boot stub

Signed-off-by: Ross Philipson <ross.philipson@oracle.com>
---
 arch/x86/boot/compressed/Makefile     |   3 +
 arch/x86/boot/compressed/early_sha1.c |  98 +++++++++++++++++++++++
 arch/x86/boot/compressed/head_64.S    |  33 ++++++++
 arch/x86/boot/compressed/tb_main.c    |  50 ++++++++++++
 arch/x86/boot/compressed/tb_stub.S    | 147 ++++++++++++++++++++++++++++++++++
 arch/x86/include/asm/sha1.h           |  16 ++++
 arch/x86/include/asm/trenchboot.h     |  68 ++++++++++++++++
 7 files changed, 415 insertions(+)
 create mode 100644 arch/x86/boot/compressed/early_sha1.c
 create mode 100644 arch/x86/boot/compressed/tb_main.c
 create mode 100644 arch/x86/boot/compressed/tb_stub.S
 create mode 100644 arch/x86/include/asm/sha1.h
 create mode 100644 arch/x86/include/asm/trenchboot.h

diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 169c2feda14a..ba0234072136 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -89,6 +89,9 @@ vmlinux-objs-$(CONFIG_EFI_STUB) += $(obj)/eboot.o $(obj)/efi_stub_$(BITS).o \
 	$(objtree)/drivers/firmware/efi/libstub/lib.a
 vmlinux-objs-$(CONFIG_EFI_MIXED) += $(obj)/efi_thunk_$(BITS).o
 
+vmlinux-objs-$(CONFIG_TRENCHBOOT_STUB) += $(obj)/tb_main.o $(obj)/tb_stub.o \
+	$(obj)/early_sha1.o
+
 # The compressed kernel is built with -fPIC/-fPIE so that a boot loader
 # can place it anywhere in memory and it will still run. However, since
 # it is executed as-is without any ELF relocation processing performed
diff --git a/arch/x86/boot/compressed/early_sha1.c b/arch/x86/boot/compressed/early_sha1.c
new file mode 100644
index 000000000000..d79918f18442
--- /dev/null
+++ b/arch/x86/boot/compressed/early_sha1.c
@@ -0,0 +1,98 @@
+/*
+ * TODO license
+ */
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <linux/string.h>
+#include <asm/sha1.h>
+#include <asm/boot.h>
+#include <asm/unaligned.h>
+
+#include "../../../../lib/sha1.c"
+
+static void early_sha1_block_fn(struct sha1_state *sst, u8 const *src,
+				int blocks)
+{
+	u32 temp[SHA_WORKSPACE_WORDS];
+
+	while (blocks--) {
+		sha_transform(sst->state, src, temp);
+		src += SHA1_BLOCK_SIZE;
+	}
+	memset(temp, 0, sizeof(temp));
+	wmb();
+}
+
+void early_sha1_init(struct sha1_state *sctx)
+{
+	sctx->state[0] = SHA1_H0;
+	sctx->state[1] = SHA1_H1;
+	sctx->state[2] = SHA1_H2;
+	sctx->state[3] = SHA1_H3;
+	sctx->state[4] = SHA1_H4;
+	sctx->count = 0;
+}
+
+void early_sha1_update(struct sha1_state *sctx,
+		       const u8 *data,
+		       unsigned int len)
+{
+	unsigned int partial = sctx->count % SHA1_BLOCK_SIZE;
+
+	sctx->count += len;
+
+	if (unlikely((partial + len) >= SHA1_BLOCK_SIZE)) {
+		int blocks;
+
+		if (partial) {
+			int p = SHA1_BLOCK_SIZE - partial;
+
+			memcpy(sctx->buffer + partial, data, p);
+			data += p;
+			len -= p;
+
+			early_sha1_block_fn(sctx, sctx->buffer, 1);
+		}
+
+		blocks = len / SHA1_BLOCK_SIZE;
+		len %= SHA1_BLOCK_SIZE;
+
+		if (blocks) {
+			early_sha1_block_fn(sctx, data, blocks);
+			data += blocks * SHA1_BLOCK_SIZE;
+		}
+		partial = 0;
+	}
+	if (len)
+		memcpy(sctx->buffer + partial, data, len);
+}
+
+void early_sha1_finalize(struct sha1_state *sctx)
+{
+	const int bit_offset = SHA1_BLOCK_SIZE - sizeof(__be64);
+	__be64 *bits = (__be64 *)(sctx->buffer + bit_offset);
+	unsigned int partial = sctx->count % SHA1_BLOCK_SIZE;
+
+	sctx->buffer[partial++] = 0x80;
+	if (partial > bit_offset) {
+		memset(sctx->buffer + partial, 0x0, SHA1_BLOCK_SIZE - partial);
+		partial = 0;
+
+		early_sha1_block_fn(sctx, sctx->buffer, 1);
+	}
+
+	memset(sctx->buffer + partial, 0x0, bit_offset - partial);
+	*bits = cpu_to_be64(sctx->count << 3);
+	early_sha1_block_fn(sctx, sctx->buffer, 1);
+}
+
+void early_sha1_finish(struct sha1_state *sctx, u8 *out)
+{
+	__be32 *digest = (__be32 *)out;
+	int i;
+
+	for (i = 0; i < SHA1_DIGEST_SIZE / sizeof(__be32); i++)
+		put_unaligned_be32(sctx->state[i], digest++);
+
+	*sctx = (struct sha1_state){};
+}
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index 64037895b085..f7bc3d7f7cfc 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -248,6 +248,22 @@ ENTRY(efi32_stub_entry)
 ENDPROC(efi32_stub_entry)
 #endif
 
+#ifdef CONFIG_TRENCHBOOT_STUB
+ENTRY(tb_stub_entry)
+	/*
+	 * On entry, %ebx has the entry abs offset to tb_stub_entry. To
+	 * find the beginning of where we are loaded, sub off from the
+	 * beginning.
+	 */
+	movl	%ebx, %ebp
+	subl	$(tb_stub_entry - startup_32), %ebp
+
+	/* More room to work in tb_stub in the text section */
+	jmp	tb_stub
+
+ENDPROC(tb_stub_entry)
+#endif
+
 	.code64
 	.org 0x200
 ENTRY(startup_64)
@@ -520,6 +536,23 @@ relocated:
 	shrq	$3, %rcx
 	rep	stosq
 
+#ifdef CONFIG_TRENCHBOOT_STUB
+	/*
+	 * Have to do the final early TB stub work in 64b area.
+	 *
+	 * *********** NOTE ***********
+	 *
+	 * Several boot params get used before we get a chance to measure
+	 * them in this call. This is a known issue and we currently don't
+	 * have a solution. One solution might be to set them in the really
+	 * early TB stub asm code but that might not work well.
+	 */
+	pushq	%rsi
+	movq	%rsi, %rdi
+	callq	tb_main
+	popq	%rsi
+#endif
+
 /*
  * Do the extraction, and jump to the new kernel..
  */
diff --git a/arch/x86/boot/compressed/tb_main.c b/arch/x86/boot/compressed/tb_main.c
new file mode 100644
index 000000000000..ed4ff21ca5d8
--- /dev/null
+++ b/arch/x86/boot/compressed/tb_main.c
@@ -0,0 +1,50 @@
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/boot.h>
+#include <asm/msr.h>
+#include <asm/processor-flags.h>
+#include <asm/asm-offsets.h>
+#include <asm/sha1.h>
+#include <asm/bootparam.h>
+#include <asm/trenchboot.h>
+
+void tb_main(u8 *bootparams)
+{
+	struct sha1_state sctx = {0};
+	u8 sha1_hash[SHA1_DIGEST_SIZE];
+	u32 cmdline_len;
+	u64 cmdline_addr;
+	u32 tb_flags;
+
+	memset(sha1_hash, 0, SHA1_DIGEST_SIZE);
+
+	/* Hash the zero page/boot params */
+	early_sha1_init(&sctx);
+	early_sha1_update(&sctx, bootparams, PAGE_SIZE);
+	early_sha1_finalize(&sctx);
+	early_sha1_finish(&sctx, &sha1_hash[0]);
+	/* TODO tpm_extend 18 */
+
+	memset(sha1_hash, 0, SHA1_DIGEST_SIZE);
+	cmdline_len = *((u32*)(bootparams + BP_CMDLINE_SIZE));
+	cmdline_addr = *((u32*)(bootparams + BP_CMD_LINE_PTR));
+
+	/* Hash the command line */
+	early_sha1_init(&sctx);
+	early_sha1_update(&sctx, (u8*)cmdline_addr, cmdline_len);
+	early_sha1_finalize(&sctx);
+	early_sha1_finish(&sctx, &sha1_hash[0]);
+	/* TODO tpm_extend 18 */
+
+	/* On Intel, have to handle TPM localities via TXT */
+	tb_flags = *((u32*)(bootparams +
+		    TRENCHBOOT_INFO_OFFSET + TB_FLAGS_OFFSET));
+	if (tb_flags & TB_FLAG_ARCH_TXT)
+	{
+		txt_write_priv_reg(TXTCR_CMD_SECRETS, 0x1);
+		(void)txt_read_priv_reg(TXTCR_E2STS);
+		txt_write_priv_reg(TXTCR_CMD_OPEN_LOCALITY1, 0x1);
+		(void)txt_read_priv_reg(TXTCR_E2STS);
+	}
+}
diff --git a/arch/x86/boot/compressed/tb_stub.S b/arch/x86/boot/compressed/tb_stub.S
new file mode 100644
index 000000000000..8960442beafb
--- /dev/null
+++ b/arch/x86/boot/compressed/tb_stub.S
@@ -0,0 +1,147 @@
+	.code32
+	.text
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/msr.h>
+#include <asm/processor-flags.h>
+#include <asm/asm-offsets.h>
+#include <asm/bootparam.h>
+#include <asm/trenchboot.h>
+
+	.code32
+ENTRY(tb_stub)
+	cli
+	cld
+
+	/* On entry %ebp has the base address from head_64.S, load the stack */
+	leal	tb_stack_end(%ebp), %esp
+
+	/* TB flag indicates TB entry mode */
+	andl	$(TB_FLAG_ACTIVE), tb_flags(%ebp)
+
+	/* On entry, only %cs is known good */
+	addl	%ebp, (tb_gdt + 2)(%ebp)
+	lgdt	tb_gdt(%ebp)
+
+	movl	$(__TB32_DS), %eax
+	movw	%ax, %ds
+	movw	%ax, %es
+	movw	%ax, %fs
+	movw	%ax, %gs
+	movw	%ax, %ss
+
+	leal	.Ltb_cs(%ebp), %eax
+	pushl	$(__TB32_CS)
+	pushl	%eax
+	lret
+
+.Ltb_cs:
+	addl	$8, %esp
+
+	/* Before going any further, make sure this is the BSP */
+	movl	$(MSR_IA32_APICBASE), %ecx
+	rdmsr
+	testl	$(MSR_IA32_APICBASE_BSP), %eax
+	jnz	.Lbsp_ok
+	ud2
+
+.Lbsp_ok:
+	/* Assume CPU is AMD to start */
+	andl	$(CPU_AMD), %edi
+
+	/* Now see if it is Intel */
+	movl	$0x0, %eax
+	cpuid
+	cmpl	$0x756e6547, %ebx # GenuineIntel?
+	jnz	.Lcpu_check_done
+	cmpl	$0x49656e69, %edx
+	jnz	.Lcpu_check_done
+	cmpl	$0x6c65746e, %ecx
+	jnz	.Lcpu_check_done
+	andl	$(CPU_INTEL), %edi
+
+.Lcpu_check_done:
+	/* Now that we know what CPU it is, do vendor specific operations */
+	testl	$(CPU_AMD), %edi
+	jz	.Ldo_amd
+
+	/* Enable SMI with GETSET[SMCTRL] */
+	xorl	%ebx, %ebx
+	movl	$(X86_GETSEC_SMCTRL), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+	/* An IRET-to-self can be used to unmask NMIs which SENTER masked */
+	leal	.Lnmi_enabled(%ebp), %eax
+	pushfl
+	pushl	$(__TB32_CS)
+	pushl	%eax
+	iret
+
+.Lnmi_enabled:
+	addl	$12, %esp
+
+	/* On Intel, the zero page address is passed in the TXT heap */
+	movl	$(TXT_PUB_CONFIG_REGS_BASE), %eax /* TXT MMIO pub regs */
+	movl	TXTCR_REG_HEAP_BASE(%eax), %ecx /* read size of BIOS data */
+	addl	%ecx, %eax /* skip over BIOS data to OS-MLE data */
+	movl	(%eax), %esi /* read zero page addr into %esi */
+
+	/* Clear the TXT error registers for a clean start of day */
+	movl	$(TXT_PRIV_CONFIG_REGS_BASE), %eax /* TXT MMIO priv regs */
+	movl	$0x0, TXTCR_ERRORCODE(%eax)
+	movl	$0xffffffff, TXTCR_ESTS(%eax)
+
+	andl	$(TB_FLAG_ARCH_TXT), tb_flags(%ebp)
+
+	jmp	.Lcpu_setup_done
+
+.Ldo_amd:
+	/*
+	 * Disable maskable interrups in EFLAGS then enable global interrupts
+	 * including SMI and NMI (GIF).
+	 */
+	cli
+	stgi
+
+	/* On AMD %esi is set up by the Landing Zone, just go on */
+	andl	$(TB_FLAG_ARCH_SKINIT), tb_flags(%ebp)
+
+.Lcpu_setup_done:
+	/*
+	 * Don't enable MCE at this point as TBOOT did. The kernel will enable
+	 * it on the BSP later when it is ready.
+	 */
+
+	/* Store flags in boot params for later use */
+	movl	%eax, (TRENCHBOOT_INFO_OFFSET + TB_FLAGS_OFFSET)(%esi)
+
+	/* Don't want the kernel keeping any segment from the TB stub */
+	andb	$~(KEEP_SEGMENTS), BP_loadflags(%esi)
+
+	/* Done, jump to normal 32b pm entry */
+	jmp	startup_32
+ENDPROC(tb_stub)
+
+	.data
+	.balign 16
+tb_gdt:
+	.word	tb_gdt_end - tb_gdt - 1
+	.long	tb_gdt
+	.word	0
+	.quad	0x00cf9a000000ffff	/* __TB32_CS */
+	.quad	0x00cf92000000ffff	/* __TB32_DS */
+tb_gdt_end:
+
+	/*
+	 * Temporary storage for flags until they can be loaded into
+	 * boot params making them available to the real kernel.
+	 */
+	.balign 4
+tb_flags:
+	.long	0
+
+	/* Small stack to work with */
+	.balign 4
+tb_stack:
+	.fill 32, 1, 0
+tb_stack_end:
diff --git a/arch/x86/include/asm/sha1.h b/arch/x86/include/asm/sha1.h
new file mode 100644
index 000000000000..0fa4bcf8da40
--- /dev/null
+++ b/arch/x86/include/asm/sha1.h
@@ -0,0 +1,16 @@
+/*
+ * TODO license
+ */
+#ifndef _ASM_X86_SHA1_H
+#define _ASM_X86_SHA1_H
+
+#include <crypto/sha.h>
+
+void early_sha1_init(struct sha1_state *sctx);
+void early_sha1_update(struct sha1_state *sctx,
+		       const u8 *data,
+		       unsigned int len);
+void early_sha1_finalize(struct sha1_state *sctx);
+void early_sha1_finish(struct sha1_state *sctx, u8 *out);
+
+#endif /* _ASM_X86_SHA1_H */
diff --git a/arch/x86/include/asm/trenchboot.h b/arch/x86/include/asm/trenchboot.h
new file mode 100644
index 000000000000..7fe016300be7
--- /dev/null
+++ b/arch/x86/include/asm/trenchboot.h
@@ -0,0 +1,68 @@
+/*
+ * TODO license
+ */
+#ifndef _ASM_X86_TRENCHBOOT_H
+#define _ASM_X86_TRENCHBOOT_H
+
+#define __TB32_CS	0x0008
+#define __TB32_DS	0x0010
+
+#define CPU_AMD			1
+#define CPU_INTEL		2
+
+#define X86_GETSEC_SMCTRL	7
+
+#define TXT_PUB_CONFIG_REGS_BASE	0xfed30000
+#define TXT_PRIV_CONFIG_REGS_BASE	0xfed20000
+#define TXTCR_STS			0x0000
+#define TXTCR_ESTS			0x0008
+#define TXTCR_ERRORCODE			0x0030
+#define TXTCR_REG_HEAP_BASE		0x0300
+#define TXTCR_CMD_OPEN_LOCALITY1	0x0380
+#define TXTCR_CMD_CLOSE_LOCALITY1	0x0388
+#define TXTCR_CMD_OPEN_LOCALITY2	0x0390
+#define TXTCR_CMD_CLOSE_LOCALITY2	0x0398
+#define TXTCR_CMD_SECRETS		0x08e0
+#define TXTCR_E2STS			0x08f0
+
+#define TRENCHBOOT_INFO_OFFSET	0x0cc
+#define TB_FLAGS_OFFSET		0x4
+#define TB_XBI_OFFSET		0x8
+
+#define BP_CMD_LINE_PTR		0x228
+#define BP_CMDLINE_SIZE		0x238
+
+#define TB_FLAG_ACTIVE		0x00000001
+#define TB_FLAG_ARCH_SKINIT	0x00000002
+#define TB_FLAG_ARCH_TXT	0x00000004
+
+#ifndef __ASSEMBLY__
+
+struct os_mle_data {
+	uint32_t zero_page_addr;
+	/* TODO more to define later, may move to a common Trenchboot header */
+} __attribute__((packed));
+
+static inline u64 txt_read_pub_reg(u32 reg)
+{
+	return *(volatile u64*)(unsigned long)(TXT_PUB_CONFIG_REGS_BASE + reg);
+}
+
+static inline u64 txt_read_priv_reg(u32 reg)
+{
+	return *(volatile u64*)(unsigned long)(TXT_PRIV_CONFIG_REGS_BASE + reg);
+}
+
+static inline void txt_write_pub_reg(u32 reg, u64 val)
+{
+	*(volatile u64*)(unsigned long)(TXT_PUB_CONFIG_REGS_BASE + reg) = val;
+}
+
+static inline void txt_write_priv_reg(u32 reg, u64 val)
+{
+	*(volatile u64*)(unsigned long)(TXT_PRIV_CONFIG_REGS_BASE + reg) = val;
+}
+
+#endif
+
+#endif /* _ASM_X86_TRENCHBOOT_H */
-- 
2.13.6

